{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Activation, Input, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  language                                            feature\n",
      "0       0         0  -43.007637, -43.007637, -43.007637, -43.007637...\n",
      "1       0         0  -43.005123, -41.460415, -38.621128, -37.740406...\n",
      "2       0         0  -39.88351, -39.199074, -38.777107, -41.36376, ...\n",
      "3       0         0  -43.11376, -41.95098, -38.71098, -38.10872, -3...\n",
      "4       0         0  -42.62169, -42.62169, -42.62169, -42.62169, -4...\n",
      "RangeIndex(start=0, stop=42068, step=1)\n",
      "42068\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data_feature_mfcc20.csv')\n",
    "print(dataset[:5])\n",
    "print(dataset.index)\n",
    "feature = []\n",
    "for idx in dataset.index:\n",
    "    feature.append(list(map(float, dataset.iloc[idx, 2].split(', '))))\n",
    "print(len(feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42068, 108) (42068, 2)\n",
      "[[-43.007637 -43.007637 -43.007637 ... -41.724716 -41.208908 -40.83515 ]\n",
      " [-43.005123 -41.460415 -38.621128 ... -43.005123 -43.005123 -43.005123]\n",
      " [-39.88351  -39.199074 -38.777107 ... -42.79019  -42.79019  -42.79019 ]\n",
      " ...\n",
      " [-45.159256 -45.159256 -45.159256 ... -36.782803 -38.635868 -39.360126]\n",
      " [-45.294586 -43.634083 -42.126347 ... -39.67121  -41.439278 -45.294586]\n",
      " [-43.625343 -40.204887 -40.095592 ... -44.909782 -41.995636 -43.011883]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = feature\n",
    "Y = dataset['gender']\n",
    "\n",
    "y_data = to_categorical(Y, num_classes=2)\n",
    "\n",
    "x = np.array(X).astype('float32')\n",
    "y = np.array(y_data)\n",
    "print(x.shape, y.shape)\n",
    "print(x[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31551, 108) (31551, 2)\n",
      "(10517, 108) (10517, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25, shuffle=True)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31551, 108, 1) (10517, 108, 1)\n"
     ]
    }
   ],
   "source": [
    "x_traincnn =np.expand_dims(x_train, axis=2)\n",
    "x_testcnn= np.expand_dims(x_test, axis=2)\n",
    "print(x_traincnn.shape, x_testcnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjw27\\Anaconda3\\envs\\voice\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(256, 5,padding='same', input_shape=(108,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "opt = tf.keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7294 - accuracy: 0.6000\n",
      "Epoch 00001: val_loss improved from inf to 0.71765, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.7294 - accuracy: 0.6000 - val_loss: 0.7177 - val_accuracy: 0.6000\n",
      "Epoch 2/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7054 - accuracy: 0.4000\n",
      "Epoch 00002: val_loss improved from 0.71765 to 0.70986, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.7054 - accuracy: 0.4000 - val_loss: 0.7099 - val_accuracy: 0.6000\n",
      "Epoch 3/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.5000\n",
      "Epoch 00003: val_loss improved from 0.70986 to 0.70530, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.6899 - accuracy: 0.5000 - val_loss: 0.7053 - val_accuracy: 0.6000\n",
      "Epoch 4/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6839 - accuracy: 0.6000\n",
      "Epoch 00004: val_loss improved from 0.70530 to 0.70197, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.6839 - accuracy: 0.6000 - val_loss: 0.7020 - val_accuracy: 0.6000\n",
      "Epoch 5/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6634 - accuracy: 0.5000\n",
      "Epoch 00005: val_loss improved from 0.70197 to 0.69901, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.6634 - accuracy: 0.5000 - val_loss: 0.6990 - val_accuracy: 0.6000\n",
      "Epoch 6/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6595 - accuracy: 0.5000\n",
      "Epoch 00006: val_loss improved from 0.69901 to 0.69856, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.6595 - accuracy: 0.5000 - val_loss: 0.6986 - val_accuracy: 0.6000\n",
      "Epoch 7/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6833 - accuracy: 0.5000\n",
      "Epoch 00007: val_loss improved from 0.69856 to 0.69761, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.6833 - accuracy: 0.5000 - val_loss: 0.6976 - val_accuracy: 0.6000\n",
      "Epoch 8/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6543 - accuracy: 0.6000\n",
      "Epoch 00008: val_loss improved from 0.69761 to 0.69662, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.6543 - accuracy: 0.6000 - val_loss: 0.6966 - val_accuracy: 0.6000\n",
      "Epoch 9/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6327 - accuracy: 0.6000\n",
      "Epoch 00009: val_loss improved from 0.69662 to 0.69550, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.6327 - accuracy: 0.6000 - val_loss: 0.6955 - val_accuracy: 0.6000\n",
      "Epoch 10/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6532 - accuracy: 0.7000\n",
      "Epoch 00010: val_loss improved from 0.69550 to 0.69478, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.6532 - accuracy: 0.7000 - val_loss: 0.6948 - val_accuracy: 0.6000\n",
      "Epoch 11/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6482 - accuracy: 0.6000\n",
      "Epoch 00011: val_loss improved from 0.69478 to 0.69425, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.6482 - accuracy: 0.6000 - val_loss: 0.6942 - val_accuracy: 0.6000\n",
      "Epoch 12/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6308 - accuracy: 0.7000\n",
      "Epoch 00012: val_loss improved from 0.69425 to 0.69405, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.6308 - accuracy: 0.7000 - val_loss: 0.6940 - val_accuracy: 0.6000\n",
      "Epoch 13/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6406 - accuracy: 0.6000\n",
      "Epoch 00013: val_loss improved from 0.69405 to 0.69361, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.6406 - accuracy: 0.6000 - val_loss: 0.6936 - val_accuracy: 0.6000\n",
      "Epoch 14/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6401 - accuracy: 0.6000\n",
      "Epoch 00014: val_loss improved from 0.69361 to 0.69250, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.6401 - accuracy: 0.6000 - val_loss: 0.6925 - val_accuracy: 0.6000\n",
      "Epoch 15/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6240 - accuracy: 0.7000\n",
      "Epoch 00015: val_loss improved from 0.69250 to 0.69134, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.6240 - accuracy: 0.7000 - val_loss: 0.6913 - val_accuracy: 0.6000\n",
      "Epoch 16/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.7000\n",
      "Epoch 00016: val_loss did not improve from 0.69134\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6280 - accuracy: 0.7000 - val_loss: 0.6921 - val_accuracy: 0.6000\n",
      "Epoch 17/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.7000\n",
      "Epoch 00017: val_loss did not improve from 0.69134\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6198 - accuracy: 0.7000 - val_loss: 0.6916 - val_accuracy: 0.6000\n",
      "Epoch 18/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6231 - accuracy: 0.6000\n",
      "Epoch 00018: val_loss did not improve from 0.69134\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6231 - accuracy: 0.6000 - val_loss: 0.6914 - val_accuracy: 0.6000\n",
      "Epoch 19/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6251 - accuracy: 0.6000\n",
      "Epoch 00019: val_loss improved from 0.69134 to 0.69124, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.6251 - accuracy: 0.6000 - val_loss: 0.6912 - val_accuracy: 0.6000\n",
      "Epoch 20/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6108 - accuracy: 0.6000\n",
      "Epoch 00020: val_loss improved from 0.69124 to 0.69037, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.6108 - accuracy: 0.6000 - val_loss: 0.6904 - val_accuracy: 0.6000\n",
      "Epoch 21/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6099 - accuracy: 0.7000\n",
      "Epoch 00021: val_loss improved from 0.69037 to 0.69037, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.6099 - accuracy: 0.7000 - val_loss: 0.6904 - val_accuracy: 0.6000\n",
      "Epoch 22/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6068 - accuracy: 0.6000\n",
      "Epoch 00022: val_loss improved from 0.69037 to 0.68781, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.6068 - accuracy: 0.6000 - val_loss: 0.6878 - val_accuracy: 0.6000\n",
      "Epoch 23/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6104 - accuracy: 0.6000\n",
      "Epoch 00023: val_loss improved from 0.68781 to 0.68716, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.6104 - accuracy: 0.6000 - val_loss: 0.6872 - val_accuracy: 0.6000\n",
      "Epoch 24/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.6000\n",
      "Epoch 00024: val_loss did not improve from 0.68716\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.6055 - accuracy: 0.6000 - val_loss: 0.6875 - val_accuracy: 0.6000\n",
      "Epoch 25/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6004 - accuracy: 0.7000\n",
      "Epoch 00025: val_loss improved from 0.68716 to 0.68615, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.6004 - accuracy: 0.7000 - val_loss: 0.6861 - val_accuracy: 0.6000\n",
      "Epoch 26/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6013 - accuracy: 0.6000\n",
      "Epoch 00026: val_loss improved from 0.68615 to 0.68580, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.6013 - accuracy: 0.6000 - val_loss: 0.6858 - val_accuracy: 0.6000\n",
      "Epoch 27/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6090 - accuracy: 0.6000\n",
      "Epoch 00027: val_loss did not improve from 0.68580\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6090 - accuracy: 0.6000 - val_loss: 0.6866 - val_accuracy: 0.6000\n",
      "Epoch 28/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5829 - accuracy: 0.7000\n",
      "Epoch 00028: val_loss improved from 0.68580 to 0.68547, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.5829 - accuracy: 0.7000 - val_loss: 0.6855 - val_accuracy: 0.6000\n",
      "Epoch 29/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5891 - accuracy: 0.6000\n",
      "Epoch 00029: val_loss improved from 0.68547 to 0.68356, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.5891 - accuracy: 0.6000 - val_loss: 0.6836 - val_accuracy: 0.6000\n",
      "Epoch 30/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5876 - accuracy: 0.7000\n",
      "Epoch 00030: val_loss did not improve from 0.68356\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5876 - accuracy: 0.7000 - val_loss: 0.6840 - val_accuracy: 0.6000\n",
      "Epoch 31/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5840 - accuracy: 0.7000\n",
      "Epoch 00031: val_loss improved from 0.68356 to 0.68349, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.5840 - accuracy: 0.7000 - val_loss: 0.6835 - val_accuracy: 0.6000\n",
      "Epoch 32/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5646 - accuracy: 0.8000\n",
      "Epoch 00032: val_loss did not improve from 0.68349\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5646 - accuracy: 0.8000 - val_loss: 0.6838 - val_accuracy: 0.6000\n",
      "Epoch 33/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.6000\n",
      "Epoch 00033: val_loss improved from 0.68349 to 0.68193, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.5776 - accuracy: 0.6000 - val_loss: 0.6819 - val_accuracy: 0.6000\n",
      "Epoch 34/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5730 - accuracy: 0.7000\n",
      "Epoch 00034: val_loss improved from 0.68193 to 0.68068, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.5730 - accuracy: 0.7000 - val_loss: 0.6807 - val_accuracy: 0.6000\n",
      "Epoch 35/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5616 - accuracy: 0.8000\n",
      "Epoch 00035: val_loss improved from 0.68068 to 0.67965, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.5616 - accuracy: 0.8000 - val_loss: 0.6796 - val_accuracy: 0.6000\n",
      "Epoch 36/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5668 - accuracy: 0.8000\n",
      "Epoch 00036: val_loss improved from 0.67965 to 0.67861, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.5668 - accuracy: 0.8000 - val_loss: 0.6786 - val_accuracy: 0.6000\n",
      "Epoch 37/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5781 - accuracy: 0.7000\n",
      "Epoch 00037: val_loss improved from 0.67861 to 0.67837, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.5781 - accuracy: 0.7000 - val_loss: 0.6784 - val_accuracy: 0.6000\n",
      "Epoch 38/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.8000\n",
      "Epoch 00038: val_loss improved from 0.67837 to 0.67723, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.5608 - accuracy: 0.8000 - val_loss: 0.6772 - val_accuracy: 0.6000\n",
      "Epoch 39/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.8000\n",
      "Epoch 00039: val_loss improved from 0.67723 to 0.67704, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.5603 - accuracy: 0.8000 - val_loss: 0.6770 - val_accuracy: 0.6000\n",
      "Epoch 40/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5576 - accuracy: 0.8000\n",
      "Epoch 00040: val_loss improved from 0.67704 to 0.67609, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.5576 - accuracy: 0.8000 - val_loss: 0.6761 - val_accuracy: 0.6000\n",
      "Epoch 41/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.8000\n",
      "Epoch 00041: val_loss improved from 0.67609 to 0.67607, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.5493 - accuracy: 0.8000 - val_loss: 0.6761 - val_accuracy: 0.6000\n",
      "Epoch 42/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5529 - accuracy: 0.8000\n",
      "Epoch 00042: val_loss improved from 0.67607 to 0.67505, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.5529 - accuracy: 0.8000 - val_loss: 0.6750 - val_accuracy: 0.6000\n",
      "Epoch 43/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5444 - accuracy: 0.9000\n",
      "Epoch 00043: val_loss did not improve from 0.67505\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5444 - accuracy: 0.9000 - val_loss: 0.6754 - val_accuracy: 0.6000\n",
      "Epoch 44/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.7000\n",
      "Epoch 00044: val_loss improved from 0.67505 to 0.67442, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.5558 - accuracy: 0.7000 - val_loss: 0.6744 - val_accuracy: 0.6000\n",
      "Epoch 45/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5483 - accuracy: 0.8000\n",
      "Epoch 00045: val_loss improved from 0.67442 to 0.67301, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.5483 - accuracy: 0.8000 - val_loss: 0.6730 - val_accuracy: 0.6000\n",
      "Epoch 46/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.9000\n",
      "Epoch 00046: val_loss did not improve from 0.67301\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5397 - accuracy: 0.9000 - val_loss: 0.6732 - val_accuracy: 0.6000\n",
      "Epoch 47/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5410 - accuracy: 0.9000\n",
      "Epoch 00047: val_loss did not improve from 0.67301\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5410 - accuracy: 0.9000 - val_loss: 0.6746 - val_accuracy: 0.6000\n",
      "Epoch 48/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5318 - accuracy: 0.8000\n",
      "Epoch 00048: val_loss improved from 0.67301 to 0.67145, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.5318 - accuracy: 0.8000 - val_loss: 0.6714 - val_accuracy: 0.6000\n",
      "Epoch 49/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.8000\n",
      "Epoch 00049: val_loss improved from 0.67145 to 0.67065, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.5278 - accuracy: 0.8000 - val_loss: 0.6706 - val_accuracy: 0.6000\n",
      "Epoch 50/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.9000\n",
      "Epoch 00050: val_loss improved from 0.67065 to 0.67052, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.5291 - accuracy: 0.9000 - val_loss: 0.6705 - val_accuracy: 0.6000\n",
      "Epoch 51/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.9000\n",
      "Epoch 00051: val_loss improved from 0.67052 to 0.66995, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.5292 - accuracy: 0.9000 - val_loss: 0.6699 - val_accuracy: 0.6000\n",
      "Epoch 52/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.9000\n",
      "Epoch 00052: val_loss did not improve from 0.66995\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5342 - accuracy: 0.9000 - val_loss: 0.6703 - val_accuracy: 0.6000\n",
      "Epoch 53/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.9000\n",
      "Epoch 00053: val_loss improved from 0.66995 to 0.66880, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.5332 - accuracy: 0.9000 - val_loss: 0.6688 - val_accuracy: 0.6000\n",
      "Epoch 54/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5109 - accuracy: 0.8000\n",
      "Epoch 00054: val_loss improved from 0.66880 to 0.66864, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.5109 - accuracy: 0.8000 - val_loss: 0.6686 - val_accuracy: 0.6000\n",
      "Epoch 55/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.9000\n",
      "Epoch 00055: val_loss improved from 0.66864 to 0.66840, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.5070 - accuracy: 0.9000 - val_loss: 0.6684 - val_accuracy: 0.6000\n",
      "Epoch 56/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5133 - accuracy: 0.9000\n",
      "Epoch 00056: val_loss improved from 0.66840 to 0.66776, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.5133 - accuracy: 0.9000 - val_loss: 0.6678 - val_accuracy: 0.6000\n",
      "Epoch 57/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.9000\n",
      "Epoch 00057: val_loss improved from 0.66776 to 0.66754, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.5077 - accuracy: 0.9000 - val_loss: 0.6675 - val_accuracy: 0.6000\n",
      "Epoch 58/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 1.0000\n",
      "Epoch 00058: val_loss did not improve from 0.66754\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5073 - accuracy: 1.0000 - val_loss: 0.6685 - val_accuracy: 0.6000\n",
      "Epoch 59/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4999 - accuracy: 0.9000\n",
      "Epoch 00059: val_loss did not improve from 0.66754\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.4999 - accuracy: 0.9000 - val_loss: 0.6681 - val_accuracy: 0.6000\n",
      "Epoch 60/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.9000\n",
      "Epoch 00060: val_loss did not improve from 0.66754\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.5052 - accuracy: 0.9000 - val_loss: 0.6681 - val_accuracy: 0.6000\n",
      "Epoch 61/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.9000\n",
      "Epoch 00061: val_loss improved from 0.66754 to 0.66650, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.5035 - accuracy: 0.9000 - val_loss: 0.6665 - val_accuracy: 0.6000\n",
      "Epoch 62/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4916 - accuracy: 0.9000\n",
      "Epoch 00062: val_loss improved from 0.66650 to 0.66551, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.4916 - accuracy: 0.9000 - val_loss: 0.6655 - val_accuracy: 0.6000\n",
      "Epoch 63/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4900 - accuracy: 0.9000\n",
      "Epoch 00063: val_loss did not improve from 0.66551\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4900 - accuracy: 0.9000 - val_loss: 0.6663 - val_accuracy: 0.6000\n",
      "Epoch 64/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4855 - accuracy: 0.9000\n",
      "Epoch 00064: val_loss improved from 0.66551 to 0.66390, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.4855 - accuracy: 0.9000 - val_loss: 0.6639 - val_accuracy: 0.6000\n",
      "Epoch 65/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.9000\n",
      "Epoch 00065: val_loss improved from 0.66390 to 0.66261, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.4870 - accuracy: 0.9000 - val_loss: 0.6626 - val_accuracy: 0.6000\n",
      "Epoch 66/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5003 - accuracy: 0.9000\n",
      "Epoch 00066: val_loss did not improve from 0.66261\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5003 - accuracy: 0.9000 - val_loss: 0.6649 - val_accuracy: 0.6000\n",
      "Epoch 67/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.9000\n",
      "Epoch 00067: val_loss did not improve from 0.66261\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4930 - accuracy: 0.9000 - val_loss: 0.6632 - val_accuracy: 0.6000\n",
      "Epoch 68/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.9000\n",
      "Epoch 00068: val_loss improved from 0.66261 to 0.66174, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.4922 - accuracy: 0.9000 - val_loss: 0.6617 - val_accuracy: 0.6000\n",
      "Epoch 69/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4817 - accuracy: 0.9000\n",
      "Epoch 00069: val_loss improved from 0.66174 to 0.66001, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.4817 - accuracy: 0.9000 - val_loss: 0.6600 - val_accuracy: 0.6000\n",
      "Epoch 70/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.9000\n",
      "Epoch 00070: val_loss did not improve from 0.66001\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4847 - accuracy: 0.9000 - val_loss: 0.6609 - val_accuracy: 0.6000\n",
      "Epoch 71/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.9000\n",
      "Epoch 00071: val_loss did not improve from 0.66001\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4826 - accuracy: 0.9000 - val_loss: 0.6635 - val_accuracy: 0.6000\n",
      "Epoch 72/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4725 - accuracy: 0.9000\n",
      "Epoch 00072: val_loss did not improve from 0.66001\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.4725 - accuracy: 0.9000 - val_loss: 0.6618 - val_accuracy: 0.6000\n",
      "Epoch 73/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4739 - accuracy: 0.9000\n",
      "Epoch 00073: val_loss improved from 0.66001 to 0.65983, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.4739 - accuracy: 0.9000 - val_loss: 0.6598 - val_accuracy: 0.6000\n",
      "Epoch 74/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4736 - accuracy: 0.9000\n",
      "Epoch 00074: val_loss improved from 0.65983 to 0.65961, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.4736 - accuracy: 0.9000 - val_loss: 0.6596 - val_accuracy: 0.6000\n",
      "Epoch 75/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4920 - accuracy: 0.9000\n",
      "Epoch 00075: val_loss did not improve from 0.65961\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.4920 - accuracy: 0.9000 - val_loss: 0.6600 - val_accuracy: 0.6000\n",
      "Epoch 76/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.9000\n",
      "Epoch 00076: val_loss did not improve from 0.65961\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4574 - accuracy: 0.9000 - val_loss: 0.6599 - val_accuracy: 0.6000\n",
      "Epoch 77/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.9000\n",
      "Epoch 00077: val_loss improved from 0.65961 to 0.65753, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.4635 - accuracy: 0.9000 - val_loss: 0.6575 - val_accuracy: 0.6000\n",
      "Epoch 78/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.9000\n",
      "Epoch 00078: val_loss did not improve from 0.65753\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4599 - accuracy: 0.9000 - val_loss: 0.6583 - val_accuracy: 0.6000\n",
      "Epoch 79/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4567 - accuracy: 0.9000\n",
      "Epoch 00079: val_loss improved from 0.65753 to 0.65621, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.4567 - accuracy: 0.9000 - val_loss: 0.6562 - val_accuracy: 0.6000\n",
      "Epoch 80/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4567 - accuracy: 0.9000\n",
      "Epoch 00080: val_loss improved from 0.65621 to 0.65484, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.4567 - accuracy: 0.9000 - val_loss: 0.6548 - val_accuracy: 0.7000\n",
      "Epoch 81/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4492 - accuracy: 1.0000\n",
      "Epoch 00081: val_loss improved from 0.65484 to 0.65482, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.4492 - accuracy: 1.0000 - val_loss: 0.6548 - val_accuracy: 0.7000\n",
      "Epoch 82/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 1.0000\n",
      "Epoch 00082: val_loss improved from 0.65482 to 0.65353, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.4377 - accuracy: 1.0000 - val_loss: 0.6535 - val_accuracy: 0.7000\n",
      "Epoch 83/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4397 - accuracy: 0.9000\n",
      "Epoch 00083: val_loss did not improve from 0.65353\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4397 - accuracy: 0.9000 - val_loss: 0.6543 - val_accuracy: 0.7000\n",
      "Epoch 84/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4566 - accuracy: 0.9000\n",
      "Epoch 00084: val_loss did not improve from 0.65353\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4566 - accuracy: 0.9000 - val_loss: 0.6544 - val_accuracy: 0.7000\n",
      "Epoch 85/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4442 - accuracy: 0.9000\n",
      "Epoch 00085: val_loss improved from 0.65353 to 0.65084, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.4442 - accuracy: 0.9000 - val_loss: 0.6508 - val_accuracy: 0.7000\n",
      "Epoch 86/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.9000\n",
      "Epoch 00086: val_loss did not improve from 0.65084\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4259 - accuracy: 0.9000 - val_loss: 0.6514 - val_accuracy: 0.7000\n",
      "Epoch 87/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4360 - accuracy: 0.9000\n",
      "Epoch 00087: val_loss improved from 0.65084 to 0.64927, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.4360 - accuracy: 0.9000 - val_loss: 0.6493 - val_accuracy: 0.7000\n",
      "Epoch 88/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4295 - accuracy: 0.9000\n",
      "Epoch 00088: val_loss improved from 0.64927 to 0.64874, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.4295 - accuracy: 0.9000 - val_loss: 0.6487 - val_accuracy: 0.7000\n",
      "Epoch 89/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.9000\n",
      "Epoch 00089: val_loss did not improve from 0.64874\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4380 - accuracy: 0.9000 - val_loss: 0.6492 - val_accuracy: 0.7000\n",
      "Epoch 90/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.9000\n",
      "Epoch 00090: val_loss improved from 0.64874 to 0.64744, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.4271 - accuracy: 0.9000 - val_loss: 0.6474 - val_accuracy: 0.7000\n",
      "Epoch 91/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.9000\n",
      "Epoch 00091: val_loss did not improve from 0.64744\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4273 - accuracy: 0.9000 - val_loss: 0.6494 - val_accuracy: 0.7000\n",
      "Epoch 92/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4157 - accuracy: 0.9000\n",
      "Epoch 00092: val_loss did not improve from 0.64744\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4157 - accuracy: 0.9000 - val_loss: 0.6481 - val_accuracy: 0.7000\n",
      "Epoch 93/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4306 - accuracy: 0.9000\n",
      "Epoch 00093: val_loss improved from 0.64744 to 0.64616, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.4306 - accuracy: 0.9000 - val_loss: 0.6462 - val_accuracy: 0.7000\n",
      "Epoch 94/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.9000\n",
      "Epoch 00094: val_loss did not improve from 0.64616\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.4213 - accuracy: 0.9000 - val_loss: 0.6469 - val_accuracy: 0.7000\n",
      "Epoch 95/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4237 - accuracy: 0.9000\n",
      "Epoch 00095: val_loss did not improve from 0.64616\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4237 - accuracy: 0.9000 - val_loss: 0.6464 - val_accuracy: 0.7000\n",
      "Epoch 96/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4148 - accuracy: 0.9000\n",
      "Epoch 00096: val_loss did not improve from 0.64616\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4148 - accuracy: 0.9000 - val_loss: 0.6489 - val_accuracy: 0.7000\n",
      "Epoch 97/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.9000\n",
      "Epoch 00097: val_loss did not improve from 0.64616\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4100 - accuracy: 0.9000 - val_loss: 0.6490 - val_accuracy: 0.7000\n",
      "Epoch 98/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4171 - accuracy: 0.9000\n",
      "Epoch 00098: val_loss did not improve from 0.64616\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.4171 - accuracy: 0.9000 - val_loss: 0.6470 - val_accuracy: 0.7000\n",
      "Epoch 99/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.9000\n",
      "Epoch 00099: val_loss did not improve from 0.64616\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.4112 - accuracy: 0.9000 - val_loss: 0.6467 - val_accuracy: 0.7000\n",
      "Epoch 100/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3945 - accuracy: 1.0000\n",
      "Epoch 00100: val_loss improved from 0.64616 to 0.64527, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.3945 - accuracy: 1.0000 - val_loss: 0.6453 - val_accuracy: 0.7000\n",
      "Epoch 101/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.9000\n",
      "Epoch 00101: val_loss improved from 0.64527 to 0.64524, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.3880 - accuracy: 0.9000 - val_loss: 0.6452 - val_accuracy: 0.7000\n",
      "Epoch 102/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3979 - accuracy: 0.9000\n",
      "Epoch 00102: val_loss did not improve from 0.64524\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3979 - accuracy: 0.9000 - val_loss: 0.6455 - val_accuracy: 0.7000\n",
      "Epoch 103/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.9000\n",
      "Epoch 00103: val_loss did not improve from 0.64524\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4036 - accuracy: 0.9000 - val_loss: 0.6459 - val_accuracy: 0.7000\n",
      "Epoch 104/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3978 - accuracy: 0.9000\n",
      "Epoch 00104: val_loss did not improve from 0.64524\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3978 - accuracy: 0.9000 - val_loss: 0.6454 - val_accuracy: 0.7000\n",
      "Epoch 105/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3943 - accuracy: 0.9000\n",
      "Epoch 00105: val_loss improved from 0.64524 to 0.64449, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.3943 - accuracy: 0.9000 - val_loss: 0.6445 - val_accuracy: 0.7000\n",
      "Epoch 106/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.9000\n",
      "Epoch 00106: val_loss did not improve from 0.64449\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3972 - accuracy: 0.9000 - val_loss: 0.6458 - val_accuracy: 0.7000\n",
      "Epoch 107/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.9000\n",
      "Epoch 00107: val_loss did not improve from 0.64449\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3822 - accuracy: 0.9000 - val_loss: 0.6453 - val_accuracy: 0.7000\n",
      "Epoch 108/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.9000\n",
      "Epoch 00108: val_loss did not improve from 0.64449\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3875 - accuracy: 0.9000 - val_loss: 0.6452 - val_accuracy: 0.7000\n",
      "Epoch 109/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.9000\n",
      "Epoch 00109: val_loss improved from 0.64449 to 0.64197, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.3698 - accuracy: 0.9000 - val_loss: 0.6420 - val_accuracy: 0.7000\n",
      "Epoch 110/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3747 - accuracy: 1.0000\n",
      "Epoch 00110: val_loss did not improve from 0.64197\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3747 - accuracy: 1.0000 - val_loss: 0.6431 - val_accuracy: 0.7000\n",
      "Epoch 111/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3748 - accuracy: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 0.64197\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3748 - accuracy: 1.0000 - val_loss: 0.6439 - val_accuracy: 0.7000\n",
      "Epoch 112/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.9000\n",
      "Epoch 00112: val_loss did not improve from 0.64197\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3724 - accuracy: 0.9000 - val_loss: 0.6427 - val_accuracy: 0.7000\n",
      "Epoch 113/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.9000\n",
      "Epoch 00113: val_loss improved from 0.64197 to 0.64134, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.3662 - accuracy: 0.9000 - val_loss: 0.6413 - val_accuracy: 0.7000\n",
      "Epoch 114/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3729 - accuracy: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 0.64134\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3729 - accuracy: 1.0000 - val_loss: 0.6463 - val_accuracy: 0.7000\n",
      "Epoch 115/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.9000\n",
      "Epoch 00115: val_loss did not improve from 0.64134\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3848 - accuracy: 0.9000 - val_loss: 0.6477 - val_accuracy: 0.7000\n",
      "Epoch 116/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3705 - accuracy: 0.9000\n",
      "Epoch 00116: val_loss did not improve from 0.64134\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3705 - accuracy: 0.9000 - val_loss: 0.6419 - val_accuracy: 0.7000\n",
      "Epoch 117/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3736 - accuracy: 0.9000\n",
      "Epoch 00117: val_loss improved from 0.64134 to 0.64104, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.3736 - accuracy: 0.9000 - val_loss: 0.6410 - val_accuracy: 0.7000\n",
      "Epoch 118/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.9000\n",
      "Epoch 00118: val_loss did not improve from 0.64104\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3612 - accuracy: 0.9000 - val_loss: 0.6449 - val_accuracy: 0.7000\n",
      "Epoch 119/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.9000\n",
      "Epoch 00119: val_loss did not improve from 0.64104\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3765 - accuracy: 0.9000 - val_loss: 0.6425 - val_accuracy: 0.7000\n",
      "Epoch 120/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3532 - accuracy: 0.9000\n",
      "Epoch 00120: val_loss did not improve from 0.64104\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3532 - accuracy: 0.9000 - val_loss: 0.6442 - val_accuracy: 0.7000\n",
      "Epoch 121/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.9000\n",
      "Epoch 00121: val_loss did not improve from 0.64104\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3602 - accuracy: 0.9000 - val_loss: 0.6419 - val_accuracy: 0.7000\n",
      "Epoch 122/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3513 - accuracy: 0.9000\n",
      "Epoch 00122: val_loss improved from 0.64104 to 0.63974, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.3513 - accuracy: 0.9000 - val_loss: 0.6397 - val_accuracy: 0.7000\n",
      "Epoch 123/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3547 - accuracy: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 0.63974\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3547 - accuracy: 1.0000 - val_loss: 0.6424 - val_accuracy: 0.7000\n",
      "Epoch 124/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.9000\n",
      "Epoch 00124: val_loss improved from 0.63974 to 0.63787, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.3549 - accuracy: 0.9000 - val_loss: 0.6379 - val_accuracy: 0.7000\n",
      "Epoch 125/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 1.0000\n",
      "Epoch 00125: val_loss did not improve from 0.63787\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3392 - accuracy: 1.0000 - val_loss: 0.6383 - val_accuracy: 0.7000\n",
      "Epoch 126/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.9000\n",
      "Epoch 00126: val_loss improved from 0.63787 to 0.63582, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.3466 - accuracy: 0.9000 - val_loss: 0.6358 - val_accuracy: 0.7000\n",
      "Epoch 127/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.9000\n",
      "Epoch 00127: val_loss did not improve from 0.63582\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3418 - accuracy: 0.9000 - val_loss: 0.6388 - val_accuracy: 0.7000\n",
      "Epoch 128/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 0.63582\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3539 - accuracy: 1.0000 - val_loss: 0.6363 - val_accuracy: 0.7000\n",
      "Epoch 129/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.9000\n",
      "Epoch 00129: val_loss improved from 0.63582 to 0.63263, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.3305 - accuracy: 0.9000 - val_loss: 0.6326 - val_accuracy: 0.7000\n",
      "Epoch 130/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3344 - accuracy: 1.0000\n",
      "Epoch 00130: val_loss did not improve from 0.63263\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3344 - accuracy: 1.0000 - val_loss: 0.6369 - val_accuracy: 0.7000\n",
      "Epoch 131/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.9000\n",
      "Epoch 00131: val_loss did not improve from 0.63263\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3348 - accuracy: 0.9000 - val_loss: 0.6386 - val_accuracy: 0.7000\n",
      "Epoch 132/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 1.0000\n",
      "Epoch 00132: val_loss did not improve from 0.63263\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3240 - accuracy: 1.0000 - val_loss: 0.6399 - val_accuracy: 0.7000\n",
      "Epoch 133/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.9000\n",
      "Epoch 00133: val_loss did not improve from 0.63263\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3214 - accuracy: 0.9000 - val_loss: 0.6367 - val_accuracy: 0.7000\n",
      "Epoch 134/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.9000\n",
      "Epoch 00134: val_loss improved from 0.63263 to 0.63157, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.3319 - accuracy: 0.9000 - val_loss: 0.6316 - val_accuracy: 0.7000\n",
      "Epoch 135/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 1.0000\n",
      "Epoch 00135: val_loss did not improve from 0.63157\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3270 - accuracy: 1.0000 - val_loss: 0.6337 - val_accuracy: 0.7000\n",
      "Epoch 136/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 1.0000\n",
      "Epoch 00136: val_loss did not improve from 0.63157\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3176 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 0.7000\n",
      "Epoch 137/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.9000\n",
      "Epoch 00137: val_loss did not improve from 0.63157\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3183 - accuracy: 0.9000 - val_loss: 0.6348 - val_accuracy: 0.7000\n",
      "Epoch 138/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.9000\n",
      "Epoch 00138: val_loss did not improve from 0.63157\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3308 - accuracy: 0.9000 - val_loss: 0.6360 - val_accuracy: 0.7000\n",
      "Epoch 139/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 1.0000\n",
      "Epoch 00139: val_loss improved from 0.63157 to 0.63085, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.2988 - accuracy: 1.0000 - val_loss: 0.6309 - val_accuracy: 0.7000\n",
      "Epoch 140/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 1.0000\n",
      "Epoch 00140: val_loss did not improve from 0.63085\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3062 - accuracy: 1.0000 - val_loss: 0.6370 - val_accuracy: 0.7000\n",
      "Epoch 141/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3038 - accuracy: 1.0000\n",
      "Epoch 00141: val_loss improved from 0.63085 to 0.62769, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.3038 - accuracy: 1.0000 - val_loss: 0.6277 - val_accuracy: 0.7000\n",
      "Epoch 142/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 1.0000\n",
      "Epoch 00142: val_loss improved from 0.62769 to 0.62617, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.3128 - accuracy: 1.0000 - val_loss: 0.6262 - val_accuracy: 0.7000\n",
      "Epoch 143/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3086 - accuracy: 1.0000\n",
      "Epoch 00143: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.3086 - accuracy: 1.0000 - val_loss: 0.6287 - val_accuracy: 0.7000\n",
      "Epoch 144/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.9000\n",
      "Epoch 00144: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.3074 - accuracy: 0.9000 - val_loss: 0.6338 - val_accuracy: 0.7000\n",
      "Epoch 145/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 1.0000\n",
      "Epoch 00145: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3078 - accuracy: 1.0000 - val_loss: 0.6379 - val_accuracy: 0.7000\n",
      "Epoch 146/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 1.0000\n",
      "Epoch 00146: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2905 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 0.7000\n",
      "Epoch 147/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 1.0000\n",
      "Epoch 00147: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2962 - accuracy: 1.0000 - val_loss: 0.6370 - val_accuracy: 0.7000\n",
      "Epoch 148/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 1.0000\n",
      "Epoch 00148: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.2825 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 0.7000\n",
      "Epoch 149/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 1.0000\n",
      "Epoch 00149: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2944 - accuracy: 1.0000 - val_loss: 0.6413 - val_accuracy: 0.7000\n",
      "Epoch 150/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2911 - accuracy: 0.9000\n",
      "Epoch 00150: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2911 - accuracy: 0.9000 - val_loss: 0.6402 - val_accuracy: 0.7000\n",
      "Epoch 151/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2870 - accuracy: 1.0000\n",
      "Epoch 00151: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2870 - accuracy: 1.0000 - val_loss: 0.6385 - val_accuracy: 0.7000\n",
      "Epoch 152/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2920 - accuracy: 1.0000\n",
      "Epoch 00152: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2920 - accuracy: 1.0000 - val_loss: 0.6379 - val_accuracy: 0.7000\n",
      "Epoch 153/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2860 - accuracy: 1.0000 - val_loss: 0.6392 - val_accuracy: 0.7000\n",
      "Epoch 154/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 1.0000\n",
      "Epoch 00154: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2775 - accuracy: 1.0000 - val_loss: 0.6332 - val_accuracy: 0.7000\n",
      "Epoch 155/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 1.0000\n",
      "Epoch 00155: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.2972 - accuracy: 1.0000 - val_loss: 0.6339 - val_accuracy: 0.7000\n",
      "Epoch 156/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 1.0000\n",
      "Epoch 00156: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2815 - accuracy: 1.0000 - val_loss: 0.6342 - val_accuracy: 0.7000\n",
      "Epoch 157/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 1.0000\n",
      "Epoch 00157: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2781 - accuracy: 1.0000 - val_loss: 0.6277 - val_accuracy: 0.7000\n",
      "Epoch 158/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 1.0000\n",
      "Epoch 00158: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2750 - accuracy: 1.0000 - val_loss: 0.6414 - val_accuracy: 0.7000\n",
      "Epoch 159/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 1.0000\n",
      "Epoch 00159: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2755 - accuracy: 1.0000 - val_loss: 0.6368 - val_accuracy: 0.7000\n",
      "Epoch 160/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2739 - accuracy: 0.9000\n",
      "Epoch 00160: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2739 - accuracy: 0.9000 - val_loss: 0.6290 - val_accuracy: 0.7000\n",
      "Epoch 161/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 1.0000\n",
      "Epoch 00161: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2796 - accuracy: 1.0000 - val_loss: 0.6328 - val_accuracy: 0.7000\n",
      "Epoch 162/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 1.0000\n",
      "Epoch 00162: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2711 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 0.7000\n",
      "Epoch 163/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 1.0000\n",
      "Epoch 00163: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.2601 - accuracy: 1.0000 - val_loss: 0.6345 - val_accuracy: 0.7000\n",
      "Epoch 164/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2596 - accuracy: 1.0000\n",
      "Epoch 00164: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2596 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.7000\n",
      "Epoch 165/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 1.0000\n",
      "Epoch 00165: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2587 - accuracy: 1.0000 - val_loss: 0.6333 - val_accuracy: 0.7000\n",
      "Epoch 166/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 1.0000\n",
      "Epoch 00166: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2642 - accuracy: 1.0000 - val_loss: 0.6428 - val_accuracy: 0.7000\n",
      "Epoch 167/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 1.0000\n",
      "Epoch 00167: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2568 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.7000\n",
      "Epoch 168/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2546 - accuracy: 1.0000\n",
      "Epoch 00168: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2546 - accuracy: 1.0000 - val_loss: 0.6478 - val_accuracy: 0.7000\n",
      "Epoch 169/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9000\n",
      "Epoch 00169: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.2580 - accuracy: 0.9000 - val_loss: 0.6346 - val_accuracy: 0.7000\n",
      "Epoch 170/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 1.0000\n",
      "Epoch 00170: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2552 - accuracy: 1.0000 - val_loss: 0.6372 - val_accuracy: 0.7000\n",
      "Epoch 171/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 1.0000\n",
      "Epoch 00171: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2619 - accuracy: 1.0000 - val_loss: 0.6375 - val_accuracy: 0.7000\n",
      "Epoch 172/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 1.0000\n",
      "Epoch 00172: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2537 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.7000\n",
      "Epoch 173/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 1.0000\n",
      "Epoch 00173: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.2480 - accuracy: 1.0000 - val_loss: 0.6342 - val_accuracy: 0.7000\n",
      "Epoch 174/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 1.0000\n",
      "Epoch 00174: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2458 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 0.7000\n",
      "Epoch 175/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2498 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 0.7000\n",
      "Epoch 176/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 1.0000\n",
      "Epoch 00176: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2425 - accuracy: 1.0000 - val_loss: 0.6295 - val_accuracy: 0.7000\n",
      "Epoch 177/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 1.0000\n",
      "Epoch 00177: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2447 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 0.7000\n",
      "Epoch 178/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 1.0000\n",
      "Epoch 00178: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2338 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 0.7000\n",
      "Epoch 179/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 1.0000\n",
      "Epoch 00179: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2387 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 0.7000\n",
      "Epoch 180/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 1.0000\n",
      "Epoch 00180: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2365 - accuracy: 1.0000 - val_loss: 0.6384 - val_accuracy: 0.7000\n",
      "Epoch 181/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 1.0000\n",
      "Epoch 00181: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2179 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 0.7000\n",
      "Epoch 182/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 1.0000\n",
      "Epoch 00182: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2368 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 0.7000\n",
      "Epoch 183/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 1.0000\n",
      "Epoch 00183: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2375 - accuracy: 1.0000 - val_loss: 0.6297 - val_accuracy: 0.7000\n",
      "Epoch 184/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 1.0000\n",
      "Epoch 00184: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2284 - accuracy: 1.0000 - val_loss: 0.6284 - val_accuracy: 0.7000\n",
      "Epoch 185/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 1.0000\n",
      "Epoch 00185: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2227 - accuracy: 1.0000 - val_loss: 0.6285 - val_accuracy: 0.7000\n",
      "Epoch 186/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 1.0000\n",
      "Epoch 00186: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2229 - accuracy: 1.0000 - val_loss: 0.6354 - val_accuracy: 0.7000\n",
      "Epoch 187/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 1.0000\n",
      "Epoch 00187: val_loss did not improve from 0.62617\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2207 - accuracy: 1.0000 - val_loss: 0.6298 - val_accuracy: 0.7000\n",
      "Epoch 188/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 1.0000\n",
      "Epoch 00188: val_loss improved from 0.62617 to 0.61988, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.2188 - accuracy: 1.0000 - val_loss: 0.6199 - val_accuracy: 0.7000\n",
      "Epoch 189/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 1.0000\n",
      "Epoch 00189: val_loss did not improve from 0.61988\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2314 - accuracy: 1.0000 - val_loss: 0.6327 - val_accuracy: 0.7000\n",
      "Epoch 190/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2170 - accuracy: 1.0000\n",
      "Epoch 00190: val_loss did not improve from 0.61988\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2170 - accuracy: 1.0000 - val_loss: 0.6216 - val_accuracy: 0.7000\n",
      "Epoch 191/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2319 - accuracy: 1.0000\n",
      "Epoch 00191: val_loss did not improve from 0.61988\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2319 - accuracy: 1.0000 - val_loss: 0.6394 - val_accuracy: 0.7000\n",
      "Epoch 192/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 1.0000\n",
      "Epoch 00192: val_loss did not improve from 0.61988\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2164 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 0.7000\n",
      "Epoch 193/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2098 - accuracy: 1.0000\n",
      "Epoch 00193: val_loss did not improve from 0.61988\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.2098 - accuracy: 1.0000 - val_loss: 0.6330 - val_accuracy: 0.7000\n",
      "Epoch 194/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 1.0000\n",
      "Epoch 00194: val_loss did not improve from 0.61988\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.2125 - accuracy: 1.0000 - val_loss: 0.6303 - val_accuracy: 0.7000\n",
      "Epoch 195/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 1.0000\n",
      "Epoch 00195: val_loss did not improve from 0.61988\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2091 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.7000\n",
      "Epoch 196/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 1.0000\n",
      "Epoch 00196: val_loss did not improve from 0.61988\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2179 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 0.7000\n",
      "Epoch 197/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2040 - accuracy: 1.0000\n",
      "Epoch 00197: val_loss improved from 0.61988 to 0.61754, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.2040 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 0.7000\n",
      "Epoch 198/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 1.0000\n",
      "Epoch 00198: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2016 - accuracy: 1.0000 - val_loss: 0.6281 - val_accuracy: 0.7000\n",
      "Epoch 199/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 1.0000\n",
      "Epoch 00199: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2125 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 0.7000\n",
      "Epoch 200/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2078 - accuracy: 1.0000\n",
      "Epoch 00200: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2078 - accuracy: 1.0000 - val_loss: 0.6258 - val_accuracy: 0.7000\n",
      "Epoch 201/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 1.0000\n",
      "Epoch 00201: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2159 - accuracy: 1.0000 - val_loss: 0.6281 - val_accuracy: 0.7000\n",
      "Epoch 202/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 1.0000\n",
      "Epoch 00202: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2035 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 0.7000\n",
      "Epoch 203/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 1.0000\n",
      "Epoch 00203: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2103 - accuracy: 1.0000 - val_loss: 0.6396 - val_accuracy: 0.7000\n",
      "Epoch 204/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 1.0000\n",
      "Epoch 00204: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1938 - accuracy: 1.0000 - val_loss: 0.6341 - val_accuracy: 0.7000\n",
      "Epoch 205/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 1.0000\n",
      "Epoch 00205: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1936 - accuracy: 1.0000 - val_loss: 0.6187 - val_accuracy: 0.7000\n",
      "Epoch 206/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1999 - accuracy: 1.0000\n",
      "Epoch 00206: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1999 - accuracy: 1.0000 - val_loss: 0.6370 - val_accuracy: 0.7000\n",
      "Epoch 207/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 1.0000\n",
      "Epoch 00207: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1946 - accuracy: 1.0000 - val_loss: 0.6371 - val_accuracy: 0.7000\n",
      "Epoch 208/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 1.0000\n",
      "Epoch 00208: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1908 - accuracy: 1.0000 - val_loss: 0.6398 - val_accuracy: 0.7000\n",
      "Epoch 209/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 1.0000\n",
      "Epoch 00209: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1919 - accuracy: 1.0000 - val_loss: 0.6366 - val_accuracy: 0.7000\n",
      "Epoch 210/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1997 - accuracy: 1.0000\n",
      "Epoch 00210: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1997 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 0.7000\n",
      "Epoch 211/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 1.0000\n",
      "Epoch 00211: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1869 - accuracy: 1.0000 - val_loss: 0.6524 - val_accuracy: 0.7000\n",
      "Epoch 212/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 1.0000\n",
      "Epoch 00212: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.1838 - accuracy: 1.0000 - val_loss: 0.6406 - val_accuracy: 0.7000\n",
      "Epoch 213/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 1.0000\n",
      "Epoch 00213: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1750 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.7000\n",
      "Epoch 214/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 1.0000\n",
      "Epoch 00214: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1858 - accuracy: 1.0000 - val_loss: 0.6497 - val_accuracy: 0.7000\n",
      "Epoch 215/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 1.0000\n",
      "Epoch 00215: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1803 - accuracy: 1.0000 - val_loss: 0.6230 - val_accuracy: 0.7000\n",
      "Epoch 216/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1921 - accuracy: 1.0000\n",
      "Epoch 00216: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.1921 - accuracy: 1.0000 - val_loss: 0.6269 - val_accuracy: 0.7000\n",
      "Epoch 217/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 1.0000\n",
      "Epoch 00217: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1782 - accuracy: 1.0000 - val_loss: 0.6296 - val_accuracy: 0.7000\n",
      "Epoch 218/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 1.0000\n",
      "Epoch 00218: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1789 - accuracy: 1.0000 - val_loss: 0.6415 - val_accuracy: 0.7000\n",
      "Epoch 219/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1805 - accuracy: 1.0000\n",
      "Epoch 00219: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1805 - accuracy: 1.0000 - val_loss: 0.6420 - val_accuracy: 0.7000\n",
      "Epoch 220/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 1.0000\n",
      "Epoch 00220: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1684 - accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 0.7000\n",
      "Epoch 221/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 1.0000\n",
      "Epoch 00221: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1765 - accuracy: 1.0000 - val_loss: 0.6403 - val_accuracy: 0.7000\n",
      "Epoch 222/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 1.0000\n",
      "Epoch 00222: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1731 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 0.7000\n",
      "Epoch 223/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 1.0000\n",
      "Epoch 00223: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1764 - accuracy: 1.0000 - val_loss: 0.6490 - val_accuracy: 0.7000\n",
      "Epoch 224/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 1.0000\n",
      "Epoch 00224: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1677 - accuracy: 1.0000 - val_loss: 0.6369 - val_accuracy: 0.7000\n",
      "Epoch 225/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 1.0000\n",
      "Epoch 00225: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1652 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 0.7000\n",
      "Epoch 226/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 1.0000\n",
      "Epoch 00226: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.1735 - accuracy: 1.0000 - val_loss: 0.6393 - val_accuracy: 0.7000\n",
      "Epoch 227/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 1.0000\n",
      "Epoch 00227: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1593 - accuracy: 1.0000 - val_loss: 0.6361 - val_accuracy: 0.7000\n",
      "Epoch 228/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 1.0000\n",
      "Epoch 00228: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1763 - accuracy: 1.0000 - val_loss: 0.6682 - val_accuracy: 0.7000\n",
      "Epoch 229/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 1.0000\n",
      "Epoch 00229: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1619 - accuracy: 1.0000 - val_loss: 0.6371 - val_accuracy: 0.7000\n",
      "Epoch 230/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 1.0000\n",
      "Epoch 00230: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1608 - accuracy: 1.0000 - val_loss: 0.6318 - val_accuracy: 0.7000\n",
      "Epoch 231/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 1.0000\n",
      "Epoch 00231: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1564 - accuracy: 1.0000 - val_loss: 0.6441 - val_accuracy: 0.7000\n",
      "Epoch 232/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 1.0000\n",
      "Epoch 00232: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1548 - accuracy: 1.0000 - val_loss: 0.6297 - val_accuracy: 0.7000\n",
      "Epoch 233/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 1.0000\n",
      "Epoch 00233: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1542 - accuracy: 1.0000 - val_loss: 0.6237 - val_accuracy: 0.7000\n",
      "Epoch 234/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 1.0000\n",
      "Epoch 00234: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1688 - accuracy: 1.0000 - val_loss: 0.6481 - val_accuracy: 0.7000\n",
      "Epoch 235/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 1.0000\n",
      "Epoch 00235: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1615 - accuracy: 1.0000 - val_loss: 0.6500 - val_accuracy: 0.7000\n",
      "Epoch 236/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 1.0000\n",
      "Epoch 00236: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1456 - accuracy: 1.0000 - val_loss: 0.6400 - val_accuracy: 0.7000\n",
      "Epoch 237/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1465 - accuracy: 1.0000\n",
      "Epoch 00237: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1465 - accuracy: 1.0000 - val_loss: 0.6371 - val_accuracy: 0.7000\n",
      "Epoch 238/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 1.0000\n",
      "Epoch 00238: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1471 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 0.7000\n",
      "Epoch 239/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1583 - accuracy: 1.0000\n",
      "Epoch 00239: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1583 - accuracy: 1.0000 - val_loss: 0.6454 - val_accuracy: 0.7000\n",
      "Epoch 240/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 1.0000\n",
      "Epoch 00240: val_loss did not improve from 0.61754\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1467 - accuracy: 1.0000 - val_loss: 0.6323 - val_accuracy: 0.7000\n",
      "Epoch 241/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1394 - accuracy: 1.0000\n",
      "Epoch 00241: val_loss improved from 0.61754 to 0.61224, saving model to model_cnn_256.h5\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.1394 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 0.7000\n",
      "Epoch 242/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 1.0000\n",
      "Epoch 00242: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1489 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 0.7000\n",
      "Epoch 243/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 1.0000\n",
      "Epoch 00243: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1508 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 0.7000\n",
      "Epoch 244/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 1.0000\n",
      "Epoch 00244: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1503 - accuracy: 1.0000 - val_loss: 0.6479 - val_accuracy: 0.7000\n",
      "Epoch 245/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 1.0000\n",
      "Epoch 00245: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1460 - accuracy: 1.0000 - val_loss: 0.6458 - val_accuracy: 0.7000\n",
      "Epoch 246/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 1.0000\n",
      "Epoch 00246: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1470 - accuracy: 1.0000 - val_loss: 0.6459 - val_accuracy: 0.7000\n",
      "Epoch 247/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 1.0000\n",
      "Epoch 00247: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1467 - accuracy: 1.0000 - val_loss: 0.6431 - val_accuracy: 0.7000\n",
      "Epoch 248/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1371 - accuracy: 1.0000\n",
      "Epoch 00248: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1371 - accuracy: 1.0000 - val_loss: 0.6470 - val_accuracy: 0.7000\n",
      "Epoch 249/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 1.0000\n",
      "Epoch 00249: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1375 - accuracy: 1.0000 - val_loss: 0.6502 - val_accuracy: 0.7000\n",
      "Epoch 250/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 1.0000\n",
      "Epoch 00250: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1374 - accuracy: 1.0000 - val_loss: 0.6482 - val_accuracy: 0.7000\n",
      "Epoch 251/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 1.0000\n",
      "Epoch 00251: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1374 - accuracy: 1.0000 - val_loss: 0.6371 - val_accuracy: 0.7000\n",
      "Epoch 252/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 1.0000\n",
      "Epoch 00252: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1401 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.7000\n",
      "Epoch 253/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 1.0000\n",
      "Epoch 00253: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1416 - accuracy: 1.0000 - val_loss: 0.6819 - val_accuracy: 0.7000\n",
      "Epoch 254/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 1.0000\n",
      "Epoch 00254: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1334 - accuracy: 1.0000 - val_loss: 0.6520 - val_accuracy: 0.7000\n",
      "Epoch 255/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 1.0000\n",
      "Epoch 00255: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1287 - accuracy: 1.0000 - val_loss: 0.6433 - val_accuracy: 0.7000\n",
      "Epoch 256/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 1.0000\n",
      "Epoch 00256: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1277 - accuracy: 1.0000 - val_loss: 0.6694 - val_accuracy: 0.7000\n",
      "Epoch 257/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 1.0000\n",
      "Epoch 00257: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1316 - accuracy: 1.0000 - val_loss: 0.6510 - val_accuracy: 0.7000\n",
      "Epoch 258/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 1.0000\n",
      "Epoch 00258: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1346 - accuracy: 1.0000 - val_loss: 0.6674 - val_accuracy: 0.7000\n",
      "Epoch 259/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 1.0000\n",
      "Epoch 00259: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1327 - accuracy: 1.0000 - val_loss: 0.6685 - val_accuracy: 0.7000\n",
      "Epoch 260/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 1.0000\n",
      "Epoch 00260: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1374 - accuracy: 1.0000 - val_loss: 0.6711 - val_accuracy: 0.7000\n",
      "Epoch 261/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 1.0000\n",
      "Epoch 00261: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1265 - accuracy: 1.0000 - val_loss: 0.6578 - val_accuracy: 0.7000\n",
      "Epoch 262/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 1.0000\n",
      "Epoch 00262: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1235 - accuracy: 1.0000 - val_loss: 0.6711 - val_accuracy: 0.7000\n",
      "Epoch 263/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 1.0000\n",
      "Epoch 00263: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1299 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.7000\n",
      "Epoch 264/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 1.0000\n",
      "Epoch 00264: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.1306 - accuracy: 1.0000 - val_loss: 0.6592 - val_accuracy: 0.7000\n",
      "Epoch 265/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 1.0000\n",
      "Epoch 00265: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1184 - accuracy: 1.0000 - val_loss: 0.6672 - val_accuracy: 0.7000\n",
      "Epoch 266/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 1.0000\n",
      "Epoch 00266: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1202 - accuracy: 1.0000 - val_loss: 0.6518 - val_accuracy: 0.7000\n",
      "Epoch 267/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 1.0000\n",
      "Epoch 00267: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1166 - accuracy: 1.0000 - val_loss: 0.6523 - val_accuracy: 0.7000\n",
      "Epoch 268/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 1.0000\n",
      "Epoch 00268: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1181 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.7000\n",
      "Epoch 269/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 1.0000\n",
      "Epoch 00269: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1238 - accuracy: 1.0000 - val_loss: 0.6621 - val_accuracy: 0.7000\n",
      "Epoch 270/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 1.0000\n",
      "Epoch 00270: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1182 - accuracy: 1.0000 - val_loss: 0.6875 - val_accuracy: 0.7000\n",
      "Epoch 271/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 1.0000\n",
      "Epoch 00271: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1190 - accuracy: 1.0000 - val_loss: 0.6550 - val_accuracy: 0.7000\n",
      "Epoch 272/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 1.0000\n",
      "Epoch 00272: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1065 - accuracy: 1.0000 - val_loss: 0.6717 - val_accuracy: 0.7000\n",
      "Epoch 273/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 1.0000\n",
      "Epoch 00273: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1181 - accuracy: 1.0000 - val_loss: 0.6701 - val_accuracy: 0.7000\n",
      "Epoch 274/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 1.0000\n",
      "Epoch 00274: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.1048 - accuracy: 1.0000 - val_loss: 0.6599 - val_accuracy: 0.7000\n",
      "Epoch 275/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 1.0000\n",
      "Epoch 00275: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 0.6717 - val_accuracy: 0.7000\n",
      "Epoch 276/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 1.0000\n",
      "Epoch 00276: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1088 - accuracy: 1.0000 - val_loss: 0.6574 - val_accuracy: 0.7000\n",
      "Epoch 277/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 1.0000\n",
      "Epoch 00277: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1071 - accuracy: 1.0000 - val_loss: 0.6636 - val_accuracy: 0.7000\n",
      "Epoch 278/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 1.0000\n",
      "Epoch 00278: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1052 - accuracy: 1.0000 - val_loss: 0.6764 - val_accuracy: 0.7000\n",
      "Epoch 279/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 1.0000\n",
      "Epoch 00279: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1041 - accuracy: 1.0000 - val_loss: 0.6327 - val_accuracy: 0.7000\n",
      "Epoch 280/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 1.0000\n",
      "Epoch 00280: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1023 - accuracy: 1.0000 - val_loss: 0.6704 - val_accuracy: 0.7000\n",
      "Epoch 281/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 1.0000\n",
      "Epoch 00281: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.1043 - accuracy: 1.0000 - val_loss: 0.6763 - val_accuracy: 0.7000\n",
      "Epoch 282/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 1.0000\n",
      "Epoch 00282: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0975 - accuracy: 1.0000 - val_loss: 0.6631 - val_accuracy: 0.7000\n",
      "Epoch 283/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 1.0000\n",
      "Epoch 00283: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0983 - accuracy: 1.0000 - val_loss: 0.6501 - val_accuracy: 0.7000\n",
      "Epoch 284/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 1.0000\n",
      "Epoch 00284: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1000 - accuracy: 1.0000 - val_loss: 0.6737 - val_accuracy: 0.7000\n",
      "Epoch 285/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 1.0000\n",
      "Epoch 00285: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0992 - accuracy: 1.0000 - val_loss: 0.6634 - val_accuracy: 0.7000\n",
      "Epoch 286/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 1.0000\n",
      "Epoch 00286: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1021 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.7000\n",
      "Epoch 287/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 1.0000\n",
      "Epoch 00287: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1078 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.7000\n",
      "Epoch 288/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 1.0000\n",
      "Epoch 00288: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0953 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 0.7000\n",
      "Epoch 289/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 1.0000\n",
      "Epoch 00289: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0922 - accuracy: 1.0000 - val_loss: 0.6656 - val_accuracy: 0.7000\n",
      "Epoch 290/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 1.0000\n",
      "Epoch 00290: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0974 - accuracy: 1.0000 - val_loss: 0.6643 - val_accuracy: 0.7000\n",
      "Epoch 291/800\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 1.0000\n",
      "Epoch 00291: val_loss did not improve from 0.61224\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0957 - accuracy: 1.0000 - val_loss: 0.6980 - val_accuracy: 0.7000\n",
      "Epoch 00291: early stopping\n"
     ]
    }
   ],
   "source": [
    "callback = [ModelCheckpoint('model_cnn_256.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto'), \n",
    "EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='min')]\n",
    "\n",
    "cnnhistory=model.fit(x_traincnn[:10], y_train[:10], batch_size=256, epochs=800, validation_data=(x_testcnn[:10], y_test[:10]), callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 108)\n",
      "(1, 108, 1)\n"
     ]
    }
   ],
   "source": [
    "gender = ['man', 'woman']\n",
    "\n",
    "loaded_model = load_model('model_cnn_mfcc20_256.h5')\n",
    "data, sr = librosa.load('ex_data/output10.wav', duration=2.5, offset=0)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sr, n_mfcc=20), axis=0)\n",
    "mfcc = pd.DataFrame(data=mfccs)\n",
    "mfcc = mfcc.stack().to_frame().T\n",
    "print(mfcc.shape)\n",
    "\n",
    "twodim = np.expand_dims(mfcc, axis=2)\n",
    "print(twodim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n",
      "predict gender: man\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(twodim, batch_size=256, verbose=1)\n",
    "label = int(result.argmax(axis=1))\n",
    "print(f\"predict gender: {gender[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h5 to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import tf2onnx\n",
    "\n",
    "model = load_model(\"model_cnn_mfcc20_256.h5\")\n",
    "tf.saved_model.save(model, \"tmp_model\")\n",
    "\n",
    "# cmd창에서\n",
    "python -m tf2onnx.convert --saved-model tmp_model --output model.onnx"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f47eeb88b421f6e2c5e411e22ba343671a13a453bb778d5bc958219e3b5230ef"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('voice': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
